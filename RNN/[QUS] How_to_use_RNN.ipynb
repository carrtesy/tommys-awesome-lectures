{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[QUS] How_to_use_RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNJPbL5UHVFeucGnpCVt1/o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sequence를 처리하기 위한 RNN \n","\n","1. 주어진 데이터를 RNN에 넣을 수 있는 형태로 만듭니다.\n","2. 기본적인 RNN 사용법 및 적용법을 익힙니다.\n","3. LSTM, GRU의 사용법 및 적용법을 익힙니다."],"metadata":{"id":"JFC0R_4A1iU0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0IYwIC091UT9"},"outputs":[],"source":["from tqdm import tqdm\n","from torch import nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import torch"]},{"cell_type":"markdown","source":["## 데이터 전처리"],"metadata":{"id":"n8JGZ6vf3ZfT"}},{"cell_type":"markdown","source":["아래의 sample data를 확인해봅시다.  \n","전체 단어 수와 pad token의 id도 아래와 같습니다."],"metadata":{"id":"etKgmrJv3dat"}},{"cell_type":"code","source":["vocab_size = 100\n","pad_id = 0\n","\n","data = [\n","  [85,14,80,34,99,20,31,65,53,86,3,58,30,4,11,6,50,71,74,13],\n","  [62,76,79,66,32],\n","  [93,77,16,67,46,74,24,70],\n","  [19,83,88,22,57,40,75,82,4,46],\n","  [70,28,30,24,76,84,92,76,77,51,7,20,82,94,57],\n","  [58,13,40,61,88,18,92,89,8,14,61,67,49,59,45,12,47,5],\n","  [22,5,21,84,39,6,9,84,36,59,32,30,69,70,82,56,1],\n","  [94,21,79,24,3,86],\n","  [80,80,33,63,34,63],\n","  [87,32,79,65,2,96,43,80,85,20,41,52,95,50,35,96,24,80]\n","]"],"metadata":{"id":"oGR8GqPn1x9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[0]"],"metadata":{"id":"QMPRPFHRo1kH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Padding 처리를 해주면서 padding 전 길이도 저장합니다."],"metadata":{"id":"2qPOGgoh3gYB"}},{"cell_type":"code","source":["max_len = len(max(data, key=len))\n","print(f\"Maximum sequence length: {max_len}\")\n","\n","valid_lens = []\n","for i, seq in enumerate(tqdm(data)):\n","  valid_lens.append(len(seq))\n","  if len(seq) < max_len:\n","    data[i] = seq + [pad_id] * (max_len - len(seq))"],"metadata":{"id":"l5ObMr393euW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data)\n","print(valid_lens)"],"metadata":{"id":"JO9yk5KH3skJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B: batch size, L: maximum sequence length\n","batch = torch.LongTensor(data)  # (B, L)\n","batch_lens = torch.LongTensor(valid_lens)  # (B)"],"metadata":{"id":"ckjoQ4Hv3wi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch.shape"],"metadata":{"id":"FLZ0T1c233eD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_lens, sorted_idx = batch_lens.sort(descending=True)\n","batch = batch[sorted_idx]"],"metadata":{"id":"zT1J4yD16HLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(batch)\n","print(batch_lens)"],"metadata":{"id":"4m16vljl6Iem"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## RNN 사용해보기"],"metadata":{"id":"egmXWzVx37ll"}},{"cell_type":"markdown","source":["RNN에 넣기 전 word embedding을 위한 embedding layer를 만듭니다."],"metadata":{"id":"Td9xwbbj4ABX"}},{"cell_type":"code","source":["embedding_size = 256\n","\n","## TODO ##\n","embedding = None\n","\n","# d_w: embedding size\n","batch_emb = embedding(batch)  # (B, L, d_w)"],"metadata":{"id":"dbtmueDK35xw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch.shape"],"metadata":{"id":"YlSTCJCcqUeK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_emb.shape"],"metadata":{"id":"7ccQfeYW8HTM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["RNN 모델 및 초기 hidden state를 정의\n","\n","- batch_emb 변수를 RNN에 넣을 예정입니다.\n","- torch 공식 문서를 참조하여, RNN 모델을 정의해보세요. \n","- input size는 어떻게 되어야 하나요?"],"metadata":{"id":"PnBnduU-4GXP"}},{"cell_type":"code","source":["hidden_size = 512  # RNN의 hidden size\n","num_layers = 1  # 쌓을 RNN layer의 개수\n","num_dirs = 1  # 1: 단방향 RNN, 2: 양방향 RNN\n","\n","rnn = nn.RNN(\n","    # TODO #\n","    input_size = None,\n","    hidden_size = hidden_size,\n","    num_layers = num_layers,\n","    bidirectional = True if num_dirs > 1 else False,\n","    batch_first = None\n",")\n","\n","h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers * num_dirs, B, d_h)"],"metadata":{"id":"2RaWZKSI4Gqd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Vanilla RNN 활용법**"],"metadata":{"id":"Q9KD-L5YC4YH"}},{"cell_type":"markdown","source":["RNN에 batch data를 넣으면 아래와 같이 2가지 output을 얻습니다.\n","\n","\n","*   `hidden_states`: 각 time step에 해당하는 hidden state들의 묶음.\n","*   `h_n`: 모든 sequence를 거치고 나온 마지막 hidden state.\n","\n","torch의 RNN 문서를 참조하여서, ``batch_emb``변수를 rnn에 input으로 넣어보세요.\n","나온 결과의 shape도 출력해보세요. "],"metadata":{"id":"qEA7MDSl4NGw"}},{"cell_type":"code","source":[""],"metadata":{"id":"Hov9BI27s9cA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["마지막 hidden state를 이용하여 text classification task에 적용할 수 있습니다."],"metadata":{"id":"xOo2Xy744hT5"}},{"cell_type":"code","source":["num_classes = 2\n","classification_layer = nn.Linear(hidden_size, num_classes)\n","\n","# C: number of classes\n","output = classification_layer(h_n.squeeze(0))  # (1, B, d_h) => (B, C)\n","print(output.shape)"],"metadata":{"id":"KtdaqR0W4PdY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각 time step에 대한 hidden state를 이용하여 token-level의 task를 수행할 수도 있습니다."],"metadata":{"id":"K9cbRbw44kXK"}},{"cell_type":"code","source":["num_classes = 5\n","entity_layer = nn.Linear(hidden_size, num_classes)\n","\n","# C: number of classes\n","output = entity_layer(hidden_states)  # (L, B, d_h) => (L, B, C)\n","print(output.shape)"],"metadata":{"id":"yv-F7KLw4kt8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **LSTM 활용법**"],"metadata":{"id":"inHpHVXz44T-"}},{"cell_type":"markdown","source":["LSTM에선 cell state가 추가됩니다.  \n","Cell state의 shape는 hidden state의 그것과 동일합니다.\n","\n","- batch_emb 변수를 LSTM에 넣을 예정입니다.\n","- torch 공식 문서를 참조하여, LSTM 모델을 정의해보세요. \n","- input size는 어떻게 되어야 하나요?"],"metadata":{"id":"kQGkzE7c485n"}},{"cell_type":"code","source":["hidden_size = 512\n","num_layers = 1\n","num_dirs = 1\n","\n","## TODO ##\n","lstm = nn.LSTM(\n","    input_size = None,\n","    hidden_size = None,\n","    num_layers = num_layers,\n","    bidirectional = True if num_dirs > 1 else False,\n","    batch_first = None\n",")\n","\n","h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers * num_dirs, B, d_h)\n","c_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers * num_dirs, B, d_h)"],"metadata":{"id":"VeIwgX_k45oG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torch의 LSTM 문서를 참조하여서, ``batch_emb``변수를 rnn에 input으로 넣어보세요.\n","나온 결과의 shape도 출력해보세요. "],"metadata":{"id":"-Oy7ZVdycFfS"}},{"cell_type":"code","source":["## TODO ##"],"metadata":{"id":"-Z9E4vG65Bbx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **GRU 사용**"],"metadata":{"id":"JcShheOD5RRG"}},{"cell_type":"markdown","source":["GRU는 cell state가 없어 RNN과 동일하게 사용 가능합니다.   \n","GRU를 이용하여 LM task를 수행해봅시다."],"metadata":{"id":"ughudT3z5TgR"}},{"cell_type":"code","source":["gru = nn.GRU(\n","    input_size=embedding_size,\n","    hidden_size=hidden_size,\n","    num_layers=num_layers,\n","    bidirectional=True if num_dirs > 1 else False\n",")"],"metadata":{"id":"hoaaK6Gn5LLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_layer = nn.Linear(hidden_size, vocab_size)"],"metadata":{"id":"cPqE9iKP6zFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_id = batch.transpose(0, 1)[0, :]  # (B)\n","hidden = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (1, B, d_h)"],"metadata":{"id":"VY3330vm6025"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for t in range(max_len):\n","  input_emb = embedding(input_id).unsqueeze(0)  # (1, B, d_w)\n","  output, hidden = gru(input_emb, hidden)  # output: (1, B, d_h), hidden: (1, B, d_h)\n","\n","  # V: vocab size\n","  output = output_layer(output)  # (1, B, V)\n","  probs, top_id = torch.max(output, dim=-1)  # probs: (1, B), top_id: (1, B)\n","\n","  print(\"*\" * 50)\n","  print(f\"Time step: {t}\")\n","  print(output.shape)\n","  print(probs.shape)\n","  print(top_id.shape)\n","\n","  input_id = top_id.squeeze(0)  # (B)"],"metadata":{"id":"LxKH3fqc7A_1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **양방향 및 여러 layer 사용**"],"metadata":{"id":"a6xnrQ5v7WB8"}},{"cell_type":"markdown","source":["이번엔 양방향 + 2개 이상의 layer를 쓸 때 얻을 수 있는 결과에 대해 알아봅니다.\n"],"metadata":{"id":"W6HEakT87ZvD"}},{"cell_type":"code","source":["num_layers = 2\n","num_dirs = 2\n","dropout=0.1\n","\n","gru = nn.GRU(\n","    input_size=embedding_size,\n","    hidden_size=hidden_size,\n","    num_layers=num_layers,\n","    dropout=dropout,\n","    bidirectional=True if num_dirs > 1 else False\n",")"],"metadata":{"id":"rsY16YFF7BaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bidirectional이 되었고 layer의 개수가 $2$로 늘었기 때문에 hidden state의 shape도 `(4, B, d_h)`가 됩니다."],"metadata":{"id":"3RBq941o7eja"}},{"cell_type":"code","source":["# d_w: word embedding size, num_layers: layer의 개수, num_dirs: 방향의 개수\n","batch_emb = embedding(batch)  # (B, L, d_w)\n","h_0 = torch.zeros((num_layers * num_dirs, batch.shape[0], hidden_size))  # (num_layers * num_dirs, B, d_h) = (4, B, d_h)\n","\n","packed_batch = pack_padded_sequence(batch_emb.transpose(0, 1), batch_lens)\n","\n","packed_outputs, h_n = gru(packed_batch, h_0)\n","print(packed_outputs)\n","print(packed_outputs[0].shape)\n","print(h_n.shape)"],"metadata":{"id":"vnX1sVRV7d6m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs, output_lens = pad_packed_sequence(packed_outputs)\n","\n","print(outputs.shape)  # (L, B, num_dirs*d_h)\n","print(output_lens)"],"metadata":{"id":"68tacMlZ7lhJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["각각의 결과물의 shape는 다음과 같습니다.\n","\n","`outputs`: `(max_len, batch_size, num_dir * hidden_size)`  \n","`h_n`: `(num_layers*num_dirs, batch_size, hidden_size)`"],"metadata":{"id":"0fEA75iy7sRK"}},{"cell_type":"code","source":["batch_size = h_n.shape[1]\n","print(h_n.view(num_layers, num_dirs, batch_size, hidden_size))\n","print(h_n.view(num_layers, num_dirs, batch_size, hidden_size).shape)"],"metadata":{"id":"BtY4oBJn7pJb"},"execution_count":null,"outputs":[]}]}