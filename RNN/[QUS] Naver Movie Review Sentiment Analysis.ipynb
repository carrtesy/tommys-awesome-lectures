{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[QUS] Naver Movie Review Sentiment Analysis.ipynb","provenance":[{"file_id":"1GH-VebLl1lktLJOvJ4a86R5l8O3dByqt","timestamp":1640508159044}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. 네이버 영화 리뷰 데이터에 대한 이해와 전처리"],"metadata":{"id":"sOcJ4cagbpzz"}},{"cell_type":"code","metadata":{"id":"c7QL7XFIkirk"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"LzxDAs1cwmXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"U0Sk6BD2weWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbRPURtckS0U"},"source":["import pandas as pd\n","import urllib.request\n","import matplotlib.pyplot as plt\n","import re\n","from konlpy.tag import Okt\n","from tqdm import tqdm\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WAL6cIYwkNqm"},"source":["# !wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","# !wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n","# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n","# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 확인하기\n","\n","- pd.read_table() 함수를 이용하여, ``ratings_train.txt``, ``ratings_test.txt`` 파일을 읽어와주세요. 변수명은 *train_data*, *test_data* 로 해주세요.\n","\n","- 생성된 pandas data frame의 처음 10개의 행을 출력해주세요.\n","\n","- 행의 길이를 출력해주세요"],"metadata":{"id":"-XhB1RLvP-EK"}},{"cell_type":"code","metadata":{"id":"hv2WLeglkV0T"},"source":["## TODO ##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 전처리 단계\n","\n","중복 제거, null값, 공백 제거, 한글과 공백 이외의 문자 제거 등을 수행합니다."],"metadata":{"id":"-lMEOGKPP0bC"}},{"cell_type":"code","source":["train_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n","train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","train_data['document'] = train_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n","train_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","train_data = train_data.dropna(how='any') # Null 값 제거\n","print('전처리 후 테스트용 샘플의 개수 :',len(train_data))"],"metadata":{"id":"r85D4XG0dTtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"al5ilRRDodDy"},"source":["test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n","test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n","test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n","test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n","test_data = test_data.dropna(how='any') # Null 값 제거\n","print('전처리 후 테스트용 샘플의 개수 :',len(test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 토큰화"],"metadata":{"id":"QyYEooVDP7Pd"}},{"cell_type":"code","source":["import pickle"],"metadata":{"id":"JM6qL9SToGjE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["토큰화 작업은 시간이 꽤 걸립니다. 그래서 피클 파일들을 준비해 두었어요. "],"metadata":{"id":"yjoN7MaEQ-BZ"}},{"cell_type":"code","source":["START_FROM_SCRATCH = False\n","\n","stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n","\n","if START_FROM_SCRATCH:\n","    # 10분쯤 걸려요..!\n","    okt = Okt()\n","    X_train = []\n","    for sentence in tqdm(train_data['document']):\n","        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n","        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n","        X_train.append(stopwords_removed_sentence)\n","    X_test = []\n","    for sentence in tqdm(test_data['document']):\n","        tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n","        stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n","        X_test.append(stopwords_removed_sentence)\n","    \n","    # X_train, X_test 객체 저장하기\n","    with open(\"X_train.pkl\", \"wb\") as f:\n","        pickle.dump(X_train, f)\n","    with open(\"X_test.pkl\", \"wb\") as f:\n","        pickle.dump(X_test, f)\n","\n","# X_train, X_test 객체 불러오기\n","with open(\"X_train.pkl\", \"rb\") as f:\n","    X_train = pickle.load(f)\n","with open(\"X_test.pkl\", \"rb\") as f:\n","    X_test = pickle.load(f)"],"metadata":{"id":"48Jf-mQcnuU-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Word to index 생성하기"],"metadata":{"id":"EyCsmXWriiEZ"}},{"cell_type":"code","source":["from collections import defaultdict"],"metadata":{"id":"r_IjbWw6jBBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_count = defaultdict(int)\n","\n","for tokens in tqdm(X_train):\n","    for token in tokens:\n","        word_count[token] += 1\n","\n","word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)"],"metadata":{"id":"2KBtYt_vjDU7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모든 단어를 다 쓰지 말고, 빈도가 가장 높은 20000개의 단어만 써봅시다. "],"metadata":{"id":"Ned2ctLIjJdR"}},{"cell_type":"code","source":["vocab_size = 20000\n","## TODO ##"],"metadata":{"id":"bLE009MvjIt9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["UNK, PAD 토큰을 추가해 봅시다."],"metadata":{"id":"dA5Z5DpUjUXO"}},{"cell_type":"code","source":["pad = 0\n","unk = 1\n","\n","pad_token = \"[PAD]\"\n","unk_token = \"[UNK]\""],"metadata":{"id":"97fN6D5mjCFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_count.insert(pad, (pad_token,-1))\n","word_count.insert(unk, (unk_token,-1))\n","vocab_size += 2\n","print(list(word_count))"],"metadata":{"id":"XoMOugqGjTp6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["w2i 딕셔너리를 정의해봅시다."],"metadata":{"id":"igsoB6Asj0KR"}},{"cell_type":"code","source":["w2i = {} \n","for pair in tqdm(word_count):\n","    if pair[0] not in w2i:\n","        w2i[pair[0]] = len(w2i)\n","print(w2i)"],"metadata":{"id":"7fmeGledh81d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keys, values = list(w2i.keys()), list(w2i.values())\n","keys[0], values[0]"],"metadata":{"id":"_lNEcyicpTd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 준비하기\n","\n","데이터로더를 쓰기 전, 연습 먼저 해봅시다."],"metadata":{"id":"GU22fQk4kG4B"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"YOH61rq9qW4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["쉬운 데이터 처리를 위해, 문장의 길이를 앞에서부터 30 tokens로 제한합시다.\n","\n","string으로 되어있는 데이터를 w2i dictionary를 활용하여 정수로 바꾸어 줍시다. \n","\n","w2i dictionary에 없다면, 어떻게 해야 할까요?\n"],"metadata":{"id":"vTrnDumDkQ97"}},{"cell_type":"code","source":["max_length = 30"],"metadata":{"id":"TvqhPpgbkQBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_int = []\n","for sentence in tqdm(X_train):\n","    ## TODO ##"],"metadata":{"id":"h5pAt0Y2iqU4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_int[0]"],"metadata":{"id":"eM2vx6-VlAqm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["torch의 pad_sequence를 활용하여, padding 을 해 봅시다. \n","\n","``batch_first`` 와 ``padding_value`` argument들을 활용해보세요. "],"metadata":{"id":"in8u9ABIkx3C"}},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"vekfmc-QqBwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## TODO ##\n","X_train_pad = None"],"metadata":{"id":"oO9Q_gJ7qF4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_pad[0], train_data[\"document\"][0]"],"metadata":{"id":"6jIUUhGdSwQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["라벨(y) 도 같이 준비합시다."],"metadata":{"id":"s2K5BfvZl60Q"}},{"cell_type":"code","metadata":{"id":"9pAvQvpo_Acb"},"source":["y_train = np.array(train_data['label'])\n","y_test = np.array(test_data['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGCTzFOJ_tpC"},"source":["len(X_train), len(y_train), len(X_test), len(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MovieReviewDataset(Dataset):\n","    def __init__(self, X, y, max_length = max_length, pad = pad):\n","        self.X = self.preprocess_data(X, max_length, pad = pad)\n","        self.y = torch.Tensor(y)\n","        \n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","    def preprocess_data(self, X, max_length = max_length, pad = pad):\n","        data = []\n","\n","        ## TODO ##\n","        #1. 최대 길이보다 길면 자르기\n","        #2. padding\n","        #3. 필요하다면, 텐서 타입으로 변경\n","        \n","        return data"],"metadata":{"id":"RSAlXEa6qVe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets = {}\n","datasets[\"train\"] = MovieReviewDataset(X_train, y_train)\n","datasets[\"test\"] = MovieReviewDataset(X_test, y_test)"],"metadata":{"id":"VCAlGHLWXLL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets[\"train\"][0]"],"metadata":{"id":"tkfwx9RfXRM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataloaders = {}\n","dataloaders[\"train\"]= DataLoader(datasets[\"train\"], batch_size=64, shuffle=True)\n","dataloaders[\"test\"]= DataLoader(datasets[\"test\"], batch_size=64, shuffle=True)"],"metadata":{"id":"ZfURVXVCsAkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(dataloaders[\"train\"]))"],"metadata":{"id":"BT7KZfI-XgWH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. LSTM으로 네이버 영화 리뷰 감성 분류하기"],"metadata":{"id":"FtGXGgP2bua5"}},{"cell_type":"markdown","source":["모델을 구성하고, device에 올려봅니다. "],"metadata":{"id":"0i4cO2uA43Bf"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"6fwcAGZKUFuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MovieReviewModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers = 1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.rnn = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, num_layers = num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        embeddings = self.embedding(x)\n","        output, (hidden, cell) = self.rnn(embeddings)\n","        final_output = torch.sigmoid(self.linear(output[:, -1, :]))\n","        return final_output"],"metadata":{"id":"67p14Ka8sH8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MovieReviewModel(vocab_size = vocab_size, embedding_dim = 100, hidden_size = 128, num_layers = 1)\n","model.to(device)"],"metadata":{"id":"jphMSaYBttvg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_x, sample_y = next(iter(dataloaders[\"train\"]))"],"metadata":{"id":"GpuzxAcKZBQv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_x.shape, sample_y.shape"],"metadata":{"id":"MFfZTgHoZDut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataloaders[\"train\"])"],"metadata":{"id":"H82pM_f2tY2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델을 training 시켜봅시다."],"metadata":{"id":"ccEHJYAz47hS"}},{"cell_type":"code","source":["learning_rate=1e-3\n","epochs = 10\n","optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","criterion = nn.BCELoss()\n","\n","model.train()\n","for e in range(1, epochs+1):\n","    train_loss = 0.0\n","    for x,y in dataloaders[\"train\"]:\n","\n","        # TODO #\n","\n","        # 1. forward\n","\n","        # 2. backward\n","        \n","    epoch_loss = None\n","    print(f\"EPOCH {e}: {epoch_loss}\")"],"metadata":{"id":"OgTR0O3rbGbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["test 데이터셋에서 정확도를 산출해봅시다."],"metadata":{"id":"4o2XB0DR4-f6"}},{"cell_type":"code","source":["with torch.no_grad():\n","  test_loss = 0.0\n","  test_correct = 0\n","  test_total = 0\n","  for x,y in dataloaders[\"test\"]:\n","\n","    ## TODO ##\n","\n","    # 1. 모델 아웃풋 산출\n","    \n","    # 2. 맞은 개수, 총 개수 업데이트\n","\n","  print(f\"accuracy: {test_correct/test_total}\")"],"metadata":{"id":"TCOdbBz-2POH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. 리뷰 예측해보기"],"metadata":{"id":"ucyY3JVpbwxK"}},{"cell_type":"code","metadata":{"id":"vbvk5X3PPExa"},"source":["def sentiment_predict(new_sentence):\n","  print(new_sentence)\n","\n","  # 전처리 과정\n","  okt = Okt()\n","  new_sentence = new_sentence.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n","  new_sentence = new_sentence.replace('^ +', \"\")\n","  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n","  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n","  new_sentence = new_sentence[:max_length]\n","  new_sentence = list(map(lambda w:w2i[w] if w in w2i else w2i['[UNK]'], new_sentence))\n","  new_sentence = torch.LongTensor(new_sentence).to(device).unsqueeze(0)\n","  \n","  print(f\"tokenized: {new_sentence}\")\n","\n","  score = float(model(new_sentence))\n","  if(score > 0.5):\n","    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n","  else:\n","    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFC0qDlO28Zg"},"source":["sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4v-MwEB29Vk"},"source":["sentiment_predict('이 영화 핵노잼 ㅠㅠ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ybiSO0142-oc"},"source":["sentiment_predict('이딴게 영화냐 ㅉㅉ')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8O25qZvf3Ao8"},"source":["sentiment_predict('감독 뭐하는 놈이냐?')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqLrvLSs3BgE"},"source":["sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다')"],"execution_count":null,"outputs":[]}]}